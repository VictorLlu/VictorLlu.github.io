<!-- Page adopted from https://github.com/adjidieng/adjidieng.github.io and https://github.com/jonbarron/website -->

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    a {
      color: #1772d0;
      text-decoration:none;
    }

    em {
      color: #1772d0;
    }

    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }

    td,th,tr,p,a {
      font-family:  'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 17px;
      font-weight: 300;
    }

    body { 
    	margin:5px 0; 
    	padding:0; 
    	font-size: 100%;
    	font-family: "PT Serif","Georgia","Helvetica Neue",Arial,sans-serif; 
    	color:#FFF;  
    	background-color:#eee;
    	line-height: 1.4em; 
    	/*background : #E4E4E4 url(bg.gif) repeat-x;*/
     	/*background: #E4E4E4 url(bg_light.gif) repeat-x; */
    	background: #FFFFFF url(bc-website-color.gif) repeat-x;
    }
    p { 
    	margin: 0 0 5px 0; 
    	padding: 0; 
    	color: #404241; 
    	background: inherit;
    	font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 95%;
      font-weight: 400;
    }

    hr {
      border: 0;
      height: 1px;
      color: #eee;
      background-color: #eee;
    }

    a { 
    	background: inherit; 
    	text-decoration:none;
    	font-size:95%;
      color: #191970;

    }

    a:hover { 
    	background: inherit;
    	text-decoration: none;

    }

    h1 { 
    	padding:0; 
    	margin:0; 
    	color: #434A55; 
    	background: inherit;
    	font-family: serif; 
    	/*font-size: 22px;*/
    	/*letter-spacing: 0px;*/
    }

    h1 a {
    	color: #191970; 
    	background: inherit;
    }

    h2 { 
    	background-color: inherit; 
    	color:#191970; 
    	margin: 10px 20px 10px 0px; 
    	padding:15px 0px 0 0px; 
    	font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 18px;
      font-weight: 500;

    }

    h2 a { 
    	/*background-color:#08152E;*/ 
    	/*background-color:#E4E4E4;*/
    	background-color:#FFFFFF;
    }

    ul { 	margin: 0 0 20px 0; 
    	padding : 0; 
    	list-style : none; 
      color: #555;
    }
    	
    li { 
    	float: left;
    	font-weight: bold;
    	margin: 10px 0 8px 0;
    	padding: 0 0 0 5px;
    	font-size: 95%%;
    	font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    	font-weight: 400;
      color: #404241;
      width: 100%;

    }

    li a { 
      font-size: 95%;
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-weight: 400;
      color: #191970; }
    li a:hover {text-decoration: none; 
      background: inherit url(select.gif) no-repeat center top;padding: 2px 4px;
    	background-position: 100% 100%;
      color: #DBBC58;
      padding: 4px 8px; 
      background-color:#191970; 
      border-radius: 25px;
      display: run-in;
    }

    strong {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      color: #191970;
    }

    heading {
      font-family:  'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 23px;
      color: #191970;
    }

    papertitle {
      font-family:  'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 17px;
      font-weight: 700
    }

    name {
      font-family:  "PT Serif","Georgia","Helvetica Neue",Arial,sans-serif; 
      font-size: 35px;
      font-weight: bold;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }

    .some_list { 
    	font-size: 93%;
    	font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    	font-weight: 400;
    	/*float: none;*/
    	list-style-type: circle;
    	padding : 0px; 
    	margin: 0 0 10px 30px; 
      }
  </style>
  <link rel="stylesheet" type="text/css" href="./stylesheet.css">
  <link rel="icon" type="image/png" href="Images/profile_pic.png">
  <title>Jiachen Lu</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Roboto:300,400,500,700,900,100italic,100,300,300italic,400italic,500italic,900italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="70%" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <p>
            <name>Jiachen Lu</name>
          </p>
        </td>
      </tr>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
              </p>
              <p>Master Student at Fudan University, Shanghai, China</p>
              <br> 
              <p> I am interested in <strong>Deep Learning</strong>, with a special focus on <strong>Computer Vision</strong> and <strong>Autonomouc Driving</strong>. My latest works focus on <strong>Efficient Transformer Theory</strong> and <strong>Vision-based 3D representation learning</strong>.</p>
              <br>
              <p>
              I received B.Eng. degree in electronic and computer engineering from <a href="https://www.ji.sjtu.edu.cn/">University of Michigan-Shanghai Jiaotong University (UM-SJTU) Joint Institute</a>. I am now a Master student in <a href="https://www.fudan.edu.cn/en/">School of Data Science, Fudan University</a>. During my Master, I conduct my research at <a href="https://fudan-zvg.github.io/">Zhang-vsion Group</a> under the supervision of <a href="https://www.robots.ox.ac.uk/~lz/">Prof. Li Zhang</a>.
              </p>
              <br>
              <p style="text-align:center">
                <a href="pdf/CV_JiachenLu.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=OECsdBsAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/jiachenlu-2454a7224/"> LinkedIn </a> &nbsp/&nbsp
                <!-- <a href="https://github.com/Andrews2017">Github</a> &nbsp/&nbsp -->
                <a href="mailto: jclu21@m.fudan.edu.cn"> Email to:</a> victor/dot/lu/dot/9901/at/gmail/dot/com
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="Images/JiachenLu_CVPR.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="Images/JiachenLu_CVPR.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody>
      </table>
 
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>News</heading>
          <ul style="list-style-type:circle">
            <li class="some_list">April 2024: I have been offered PhD offers from <strong>CMU, UC Berkeley, EPFL, etc.</strong> in 2024 Fall PhD Application. </li>
            <li class="some_list">March 2024: My work on <strong> Efficient Transformer Theory </strong> "Softmax-free Linear Transformers" appears on <strong>IJCV 2024 March</strong>.</li>
            <li class="some_list">Feb 2024: My personal citation excceed <strong>3000</strong> on google scholar!</li>
            <li class="some_list">July 2023: My work on <strong>HD-map of Autonomous Driving </strong> "Translating Images to Road Network: A Non-Autoregressive Sequence-to-Sequence Approach" is accepted as <strong>ICCV 2023 Oral</strong>.</li>
            <li class="some_list">June 2023: My work on <strong>3D Temporal Detection </strong> "SUIT: Learning Significance-guided Information for 3D Temporal Detection" is accepted by <strong>IROS 2023</strong>.</li>
            <li class="some_list">March 2023: My work on <strong>generative perception model</strong> <a href="https://arxiv.org/pdf/2303.11316">"Geneartive Semantic Segmentation"</a> is accepted by <strong>CVPR 2023</strong>.</li>
            <li class="some_list">Jan 2023: My work on <strong>mobile Transformer</strong> <a href="https://openreview.net/pdf?id=-qg8MQNrxZw">"SeaFormer: Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation"</a> is accepted by <strong>ICLR 2023</strong>.</li>
            <li class="some_list">Jan 2023: <strong>E2EAD CVPR2023:</strong> 1st workshop on End-to-End Autonomous Driving:
              Perception, Prediction, Planning and Simulation is now open for <a href="https://cmt3.research.microsoft.com/E2EAD2023"><strong>submission</strong></a> on CMT.</li>
            <li class="some_list">Dec 2022: Committee member of <strong>E2EAD CVPR2023</strong> </li>
          </ul>         
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p> I have my interest in computervision. I have worked on semantic segmentation, Transformer, efficient Transformer, vision-based 3D detection, Bird's Eye View road segmentation.
          </p>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
        <tr>
          <td width="25%">
            <img src='Images/soft_ijcv.webp' width="280" height="150">
          </td>
          <td valign="top" width="75%">
            <p>
              <a href="https://link.springer.com/article/10.1007/s11263-024-02035-5">
                <papertitle>Softmax-Free Linear Transformers</papertitle>
              </a>
              <br>
              <strong>Jiachen Lu</strong>, Junge Zhang, Xiatian Zhu, Jianfeng Feng, Tao Xiang, Li Zhang
              <br>
              <em>IJCV 2024 March</em>
              <br>
              <a href="hthttps://link.springer.com/article/10.1007/s11263-024-02035-5">Paper</a>/<a href="https://github.com/fudan-zvg/SOFT">Code</a>
            </p>
            <p>Vision transformers (ViTs) have pushed the state-of-the-art for visual perception tasks. The self-attention mechanism underpinning the strength of ViTs has a quadratic complexity in both computation and memory usage. This motivates the development of approximating the self-attention at linear complexity. However, an in-depth analysis in this work reveals that existing methods are either theoretically flawed or empirically ineffective for visual recognition. We identify that their limitations are rooted in the inheritance of softmax based self-attention during approximations, that is, normalizing the scaled dot-product between token feature vectors using the softmax function. ......
            </p>
          </td>
        </tr>
        <tr>
          <td width="25%">
            <img src='Images/sagent.png' width="280" height="200">
          </td>
          <td valign="top" width="75%">
            <p>
              <a href="https://openreview.net/pdf/e2125100cd95af863e14a61b29fc10648a1175ac.pdf">
                <papertitle>Self-organizing Agents in Open-ended Environments</papertitle>
              </a>
              <br>
              Jiaqi Chen, Yuxian Jiang, <strong>Jiachen Lu</strong>, Li Zhang
              <br>
              <em>ICLR 2024 workshop on Large Language Model (LLM) Agent</em> 
              <br>
              <a href="https://openreview.net/pdf/e2125100cd95af863e14a61b29fc10648a1175ac.pdf">Paper</a>/<a href="https://github.com/fudan-zvg/S-Agents">Code</a>
            </p>
            <p>Leveraging large language models (LLMs), autonomous agents have significantly improved, gaining the ability to handle a variety of tasks. In open-ended settings, optimizing collaboration for efficiency and effectiveness demands flexible adjustments. Despite this, current research mainly emphasizes fixed, task-oriented workflows and overlooks agent-centric organizational structures. Drawing inspiration from human organizational behavior, we introduce a self-organizing agent system (S-Agents) with a "tree of agents" structure for dynamic workflow, an "hourglass agent architecture" for balancing information priorities, and a "non-obstructive collaboration" method to allow asynchronous task execution among agents. ......
            </p>
          </td>
        <tr>
          <td width="25%">
            <img src='Images/wovogen.png' width="280" height="130">
          </td>
          <td valign="top" width="75%">
            <p>
              <a href="https://arxiv.org/pdf/2312.02934">
                <papertitle>WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation</papertitle>
              </a>
              <br>
              <strong>Jiachen Lu</strong>, Ze Huang, Zeyu Yang, Jiahui Zhang, Li Zhang
              <br>
              <em>arXiv</em> 
              <br>
              <a href="https://arxiv.org/pdf/2312.02934">Paper</a>
            </p>
            <p>Generating multi-camera street-view videos is critical for augmenting autonomous driving datasets, addressing the urgent demand for extensive and varied data. Due to the limitations in diversity and challenges in handling lighting conditions, traditional rendering-based methods are increasingly being supplanted by diffusion-based methods. However, a significant challenge in diffusion-based methods is ensuring that the generated sensor data preserve both intra-world consistency and inter-sensor coherence. To address these challenges, we combine an additional explicit world volume and propose the World Volume-aware Multi-camera Driving Scene Generator (WoVoGen). ......
            </p>
          </td>
        </tr>
        <tr>
          <td width="25%">
            <img src='Images/pgc.png' width="280" height="230">
          </td>
          <td valign="top" width="75%">
            <p>
              <a href="https://openreview.net/pdf/c0909bcd147c795dc8841fbfd0cd850bdd80e2d3.pdf">
                <papertitle>Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping</papertitle>
              </a>
              <br>
              Zijie Pan, <strong>Jiachen Lu</strong>, Xiatian Zhu, Li Zhang
              <br>
              <em>ICLR 2024</em> 
              <br>
              <a href="https://openreview.net/pdf/c0909bcd147c795dc8841fbfd0cd850bdd80e2d3.pdf">Paper</a>
            </p>
            <p>High-resolution 3D object generation remains a challenging task primarily due to the limited availability of comprehensive annotated training data. Recent advancements have aimed to overcome this constraint by harnessing image generative models, pretrained on extensive curated web datasets, using knowledge transfer techniques like Score Distillation Sampling (SDS). Efficiently addressing the requirements of high-resolution rendering often necessitates the adoption of latent representation-based models, such as the Latent Diffusion Model (LDM). In this framework, a significant challenge arises: To compute gradients for individual image pixels, it is necessary to backpropagate gradients from the designated latent space through the frozen components of the image model, such as the VAE encoder used within LDM. However, this gradient propagation pathway has never been optimized, remaining uncontrolled during training. We find that the unregulated gradients adversely affect the 3D model's capacity in acquiring texture-related information from the image generative model, leading to poor quality appearance synthesis. ......
            </p>
          </td>
        </tr>
        <tr>
          <td width="25%">
            <img src='Images/rntr.png' width="280" height="230">
          </td>
          <td valign="top" width="75%">
            <p>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Translating_Images_to_Road_Network_A_Non-Autoregressive_Sequence-to-Sequence_Approach_ICCV_2023_paper.pdf">
                <papertitle>Translating Images to Road Network: A Non-Autoregressive Sequence-to-Sequence Approach</papertitle>
              </a>
              <br>
              <strong>Jiachen Lu</strong>, Renyuan Peng, Xinyue Cai, Hang Xu, Hongyang Li, Feng Wen, Wei Zhang, Li Zhang
              <br>
              <em>ICCV 2023</em> <strong>[Oral]</strong>
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Translating_Images_to_Road_Network_A_Non-Autoregressive_Sequence-to-Sequence_Approach_ICCV_2023_paper.pdf">Paper</a>/<a href="https://github.com/fudan-zvg/RoadNetworkTRansformer">Code</a>
            </p>
            <p>The extraction of road network is essential for the generation of high-definition maps since it enables the precise localization of road landmarks and their interconnections. However, generating road network poses a significant challenge due to the conflicting underlying combination of Euclidean (eg, road landmarks location) and non-Euclidean (eg, road topological connectivity) structures. Existing methods struggle to merge the two types of data domains effectively, but few of them address it properly. Instead, our work establishes a unified representation of both types of data domain by projecting both Euclidean and non-Euclidean data into an integer series called RoadNet Sequence. ......
            </p>
          </td>
        </tr>
        <tr>
          <td width="25%">
            <img src='Images/suit.png' width="280" height="150">
          </td>
          <td valign="top" width="75%">
            <p>
              <a href="https://arxiv.org/pdf/2307.01807">
                <papertitle>SUIT: Learning Significance-guided Information for 3D Temporal Detection</papertitle>
              </a>
              <br>
              Zheyuan Zhou, <strong>Jiachen Lu</strong>, Yihan Zeng, Hang Xu, Li Zhang
              <br>
              <em>IROS 2023</em>  <strong>[Oral]</strong>
            </p>
            <p>3D object detection from LiDAR point cloud is of critical importance for autonomous driving and robotics. While sequential point cloud has the potential to enhance 3D perception through temporal information, utilizing these temporal features effectively and efficiently remains a challenging problem. Based on the observation that the foreground information is sparsely distributed in LiDAR scenes, we believe sufficient knowledge can be provided by sparse format rather than dense maps. To this end, we propose to learn Significance-gUided Information for 3D Temporal detection (SUIT), which simplifies temporal information as sparse features for information fusion across frames. ......
            </p>
          </td>
        </tr>
      <tr>
        <td width="25%">
          <img src='Images/gss.png' width="280" height="250">
        </td>
        <td valign="top" width="75%">
          <p>
            <a href="https://arxiv.org/pdf/2303.11316">
              <papertitle>Generative Semantic Segmentation</papertitle>
            </a>
            <br>
            Jiaqi Chen, <strong>Jiachen Lu</strong>, Xiatian Zhu, Li Zhang
            <br>
            <em>CVPR 2023</em>
            <br>
            <a href="https://arxiv.org/pdf/2303.11316">Paper</a>/<a href="https://github.com/fudan-zvg/GSS">Code</a>
          </p>
          <p>We present Generative Semantic Segmentation (GSS), a generative learning approach for semantic segmentation. Uniquely, we cast semantic segmentation as an image-conditioned mask generation problem. This is achieved by replacing the conventional per-pixel discriminative learning with a latent prior learning process. Specifically, we model the variational posterior distribution of latent variables given the segmentation mask. To that end, the segmentation mask is expressed with a special type of image (dubbed as maskige). This posterior distribution allows to generate segmentation masks unconditionally. To achieve semantic segmentation on a given image, we further introduce a conditioning network. It is optimized by minimizing the divergence between the posterior distribution of maskige (i.e., segmentation masks) and the latent prior distribution of input training images. Extensive experiments on standard benchmarks show that our GSS can perform competitively to prior art alternatives in the standard semantic segmentation setting, whilst achieving a new state of the art in the more challenging cross-domain setting.</p>
        </td>
      </tr>
      <tr>
        <td width="25%">
          <img src='Images/seaformer.png' width="280" height="150">
        </td>
        <td valign="top" width="75%">
          <p>
            <a href="https://openreview.net/pdf?id=-qg8MQNrxZw">
              <papertitle>SeaFormer: Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation</papertitle>
            </a>
            <br>
            Qiang Wan, Zilong_Huang, <strong>Jiachen Lu</strong>, Gang YU, Li Zhang
            <br>
            <em>ICLR 2023</em>
            <br>
            <a href="https://openreview.net/pdf?id=-qg8MQNrxZw">Paper</a>/<a href="https://github.com/fudan-zvg/SeaFormer">Code</a>
            <!-- <a href="https://github.com/fudan-zvg/SOFT">Code</a> -->
          </p>
          <p>Since the introduction of Vision Transformers, the landscape of many computer vision tasks (e.g., semantic segmentation), which has been overwhelmingly dominated by CNNs, recently has significantly revolutionized. However, the computational cost and memory requirement render these methods unsuitable on the mobile device, especially for the high resolution per-pixel semantic segmentation task. In this paper, we introduce a new method squeeze-enhanced Axial Transformer (SeaFormer) for mobile semantic segmentation. Specifically, we design a generic attention block characterized by the formulation of squeeze Axial and spatial enhancement. It can be further used to create a family of backbone architectures with superior cost-effectiveness. ......

          </p>
        </td>
      </tr>
      <tr>
        <td width="25%">
          <img src='Images/ego3rt.png' width="280" height="250">
        </td>
        <td valign="top" width="75%">
          <p>
            <a href="https://arxiv.org/pdf/2206.04042">
              <papertitle>Learning Ego 3D Representation as Ray Tracing</papertitle>
            </a>
            <br>
            <strong>Jiachen Lu</strong>, Zheyuan Zhou, Xiatian Zhu, Hang Xu, Li Zhang
            <br>
            <em>17th European Conference on Computer Vision (ECCV2022)</em>
            <br>
            <a href="https://arxiv.org/pdf/2206.04042">Paper</a>/
            <a href="https://github.com/fudan-zvg/Ego3RT">Code</a>
          </p>
          <p>A self-driving perception model aims to extract 3D semantic representations from multiple cameras collectively into the bird's-eye-view (BEV) coordinate frame of the ego car in order to ground downstream planner. Existing perception methods often rely on error-prone depth estimation of the whole scene or learning sparse virtual 3D representations without the target geometry structure, both of which remain limited in performance and/or capability. In this paper, we present a novel end-to-end architecture for ego 3D representation learning from an arbitrary number of unconstrained camera views. Inspired by the ray tracing principle, we design a polarized grid of "imaginary eyes" as the learnable ego 3D representation and formulate the learning process with the adaptive attention mechanism in conjunction with the 3D-to-2D projection. ......

          </p>
        </td>
      </tr>
      <tr>
        <td width="25%">
          <img src='Images/soft_eg.png' width="280" height="250">
        </td>
        <td valign="top" width="75%">
          <p>
            <a href="https://proceedings.neurips.cc/paper/2021/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Paper.pdf">
              <papertitle>SOFT: softmax-free transformer with linear complexity</papertitle>
            </a>
            <br>
            <strong>Jiachen Lu</strong>, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, Li Zhang
            <br>
            <em>NeurIPS2021</em> <strong>[Spotlight]</strong>
            <br>
            <a href="https://proceedings.neurips.cc/paper/2021/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Paper.pdf">Paper</a>/
            <a href="https://github.com/fudan-zvg/SOFT">Code</a>
          </p>
          <p>Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. ......

          </p>
        </td>
      </tr>
      <tr>
        <td width="25%">
          <img src='Images/setr.png' width="280" height="250">
        </td>
        <td valign="top" width="75%">
          <p>
            <a href="http://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.pdf">
              <papertitle>Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</papertitle>
            </a>
            <br>
            Sixiao Zheng, <strong>Jiachen Lu</strong>, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, Li Zhang
            <br>
            <em>CVPR 2021</em> <strong>[Cited by 3000+]</strong>
            <br>
            <a href="http://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.pdf">Paper</a>/
            <a href="https://github.com/fudan-zvg/SETR">Code</a>
          </p>
          <p>Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (ie, without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.</p>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Awards and Honors</heading>
          <ul style="list-style-type:circle">
            <li class="some_list">2024: <strong>Shanghai Outstanding Graduate (上海市优秀硕士毕业生)</strong>.</li>
            <li class="some_list">2023: 2022-2023 <strong>China National Scholarship (国家奖学金)</strong>.</li>
            <li class="some_list">2022: 2021-2022 <strong>China National Scholarship (国家奖学金)</strong>.</li>
            <li class="some_list">2021: <strong>Shanghai Outstanding Graduate (上海市优秀毕业生)</strong>.</li>
            <li class="some_list">2020: 2019-2020 John Wu & Jane Sun Sunshine Scholarship of SJTU.</li>
            <li class="some_list">2019: 3rd Prize of Formula Student Autonomous China.</li>
            <li class="some_list">2019: 2018-2019 John Wu & Jane Sun Sunshine Scholarship of SJTU.</li>
            <li class="some_list">2019: 2018-2019 <strong>China National Scholarship (国家奖学金)</strong>.</li>
            <li class="some_list">2018: 2017-2018 Yuliming Scholarship of SJTU.</li>
          </ul>         
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading>Past News</heading>
            <ul style="list-style-type:circle">
              <li class="some_list">Dec 2022: My personal citation excceed <strong>1000</strong> on google scholar!</li>
              <li class="some_list">Oct 2022: Awarded 2021-2022 <strong>China National Scholarship</strong>.</li>
              <li class="some_list">July 2022: My latests work on Transformer theory with respective of <strong>graph spectrum</strong> <a href="https://arxiv.org/pdf/2207.03341">, "Softmax-free Linear Transformers"</a> is now available on arxiv.</li>
              <li class="some_list">July 2022: My paper <a href="https://arxiv.org/pdf/2206.04042">"Learning Ego 3D Representation as Ray Tracing"</a> has been accepted by <a href="https://eccv2022.ecva.net/"><strong>ECCV2022</strong></a>.</li>
              <li class="some_list">April 2022: My talk on <strong>Efficient Transformer</strong> has been published on <a href="https://www.techbeat.net/talk-info?id=650">TechBeat</a>. </li>
              <li class="some_list">Sep 2021: My paper <a href="https://proceedings.neurips.cc/paper/2021/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Paper.pdf">"SOFT: softmax-free transformer with linear complexity"</a> has been accepted as <strong>Spotlight</strong> by <a href="https://nips.cc/Conferences/2021"><strong>NeurIPS2021</strong></a>.</li>
              <li class="some_list">May 2022: My personal citation excceed 500 on google scholar!</li>
              <li class="some_list">Aug 2021: Received a B.Eng. degree from Shanghai Jiaotong University.</li>
              <li class="some_list">Feb 2021: My paper <a href="http://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.pdf">"Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers"</a> has been accepted by <a href="https://cvpr2021.thecvf.com/"><strong>CVPR2021</strong></a>.</li>
              <li class="some_list">June 2021: Awarded Shanghai Outstanding Graduate.</li>
              <li class="some_list">Dec 2020: Starting my internship at <a href="http://dev3.noahlab.com.hk/">Noah's Ark Lab</a>.</li>
              <li class="some_list">Sep 2019: Awarded 2018-2019 <strong>China National Scholarship</strong>.</li>
            </ul>         
          </td>
        </tr>
        </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Languages</heading>
          <ul style="list-style-type:circle">
            <li class="some_list"> Strong reading, writing, speaking and listening competencies for <strong>Mandarin Chinese</strong> and <strong>English</strong>.</li>
            <li class="some_list"> François, 日本語 and Deutsch learner. </li>
          </ul>         
        </td>
      </tr>
      </table>
    </td>
    </tr>
  </table>
  </body>
</html>
